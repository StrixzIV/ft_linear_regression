{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7f7aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils.ZScoreScaler import ZScoreScaler\n",
    "from utils.LinearRegressionModel import LinearRegressionModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20e379d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/housing.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6bb5e5",
   "metadata": {},
   "source": [
    "### How the training works?\n",
    "\n",
    "From this normal linear equation:\n",
    "\n",
    "$$f(x) = mx + c$$\n",
    "\n",
    "We have 2 tunable parameters to work with which are `m` (slope) and `c` (y-intercept).\n",
    "\n",
    "To properly tune it, we can use **Scholastic Gradient Descent (SGD)** to updates model parameters which uses the entire dataset for each update by processing data in smaller as random chunks.\n",
    "\n",
    "To do a gradient descent, we need to construct an error gredients for each parameters to find the global minima which is our target.\n",
    "\n",
    "For this project, the subject requested us to use **Mean Squared Error**\n",
    "\n",
    "$$\n",
    "E_{MSE} = \\frac{1}{n} \\cdot \\sum^n_{i=1}((y_i - \\hat y_i)^2)\n",
    "$$\n",
    "\n",
    "Plugging in the error, we can construct an error gradient for each parameters like this:\n",
    "\n",
    "$$\n",
    "\\nabla E(m, c) = \\left\\langle\\frac{\\partial E}{\\partial m}, \\frac{\\partial E}{\\partial c}\\right\\rangle\n",
    "$$\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial m} = \\frac{2}{n} \\sum_{i=1}^{n}((y_i - \\hat{y_i}) \\cdot x_i)$$\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial c} = \\frac{2}{n} \\sum_{i=1}^{n}(y_i - \\hat{y_i})$$\n",
    "\n",
    "To update the parameters, we adjust each one of them by the rate of change in error respect to that parameter times learning rate $\\alpha$ to adjust how fast we converge to the minima\n",
    "\n",
    "$$\n",
    "\\left\\langle m_{j + 1}, c_{j + 1} \\right\\rangle = \\left\\langle m_{j}, c_{j} \\right\\rangle - \\alpha \\cdot \\nabla E(m, c)\n",
    "$$\n",
    "\n",
    "We got:\n",
    "\n",
    "$$\n",
    "m_{j + 1} = m_{j} - \\alpha \\cdot \\frac{\\partial E}{\\partial m}\n",
    "$$\n",
    "\n",
    "$$\n",
    "c_{j + 1} = c_{j} - \\alpha \\cdot \\frac{\\partial E}{\\partial c}\n",
    "$$\n",
    "\n",
    "Substituting the partial derivatives, we got:\n",
    "\n",
    "$$\n",
    "m_{j + 1} = m_{j} - \\frac{\\alpha}{n} \\sum_{i=1}^{n}((y_i - \\hat{y_i}) \\cdot x_i)\n",
    "$$\n",
    "\n",
    "$$\n",
    "c_{j + 1} = c_{j} - \\frac{\\alpha}{n} \\sum_{i=1}^{n}(y_i - \\hat{y_i})\n",
    "$$\n",
    "\n",
    "**Note:** Because the learning rate is a constant, we can absorb 2 in the equation with the $\\alpha$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959845b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_label = 'Size'\n",
    "y_label = 'Rent'\n",
    "\n",
    "model = LinearRegressionModel(epoch=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee478798",
   "metadata": {},
   "source": [
    "### Why do we need to do Z-score normalization here?\n",
    " \n",
    "If we take a look into our data, we can see that's there are a significant difference in the range of values in two columns we use for training our model which could potentially causes our model to bias against one range of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b6d1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 6), sharey=True)\n",
    "\n",
    "# Plot for x_label\n",
    "data_x = pd.DataFrame({'value': df[x_label].dropna(), 'dummy': 'x'})\n",
    "sns.violinplot(data=data_x, x='dummy', y='value', ax=ax1, color='#3b82f6', inner='box')\n",
    "ax1.set_title(f'{x_label}\\nDistribution', fontweight='bold')\n",
    "ax1.set_xlabel('')\n",
    "ax1.set_ylabel('Value', fontweight='bold')\n",
    "ax1.tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)\n",
    "\n",
    "# Plot for y_label\n",
    "data_y = pd.DataFrame({'value': df[y_label].dropna(), 'dummy': 'y'})\n",
    "sns.violinplot(data=data_y, x='dummy', y='value', ax=ax2, color='#ef4444', inner='box')\n",
    "ax2.set_title(f'{y_label}\\nDistribution', fontweight='bold')\n",
    "ax2.set_xlabel('')\n",
    "ax2.set_ylabel('')\n",
    "ax2.tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)\n",
    "\n",
    "# Add grid\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.suptitle(f'Side-by-Side Distribution Comparison: {x_label} vs {y_label}', \n",
    "                fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818d6223",
   "metadata": {},
   "source": [
    "To solve this problem, we can do **Data Normalization** which compress all values into the same range which in this case we use **Z-score normalization** where each data point in the dataset is represented based on how far they deviate from the mean, measured in units of standard deviation.\n",
    "\n",
    "We can calculate Z-score for each data point with:\n",
    "\n",
    "$$\n",
    "z = \\frac{x - \\mu}{\\sigma}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* **x** stands for the data point\n",
    "* **$\\mu$** stands for the mean of the dataset\n",
    "* **$\\sigma$** stands for the standard deviation of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bebabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_x = ZScoreScaler()\n",
    "scaler_y = ZScoreScaler()\n",
    "\n",
    "scaler_x.fit(df[x_label])\n",
    "scaler_y.fit(df[y_label])\n",
    "\n",
    "df[x_label] = scaler_x.transform(df[x_label])\n",
    "df[y_label] = scaler_y.transform(df[y_label])\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3366a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 6), sharey=True)\n",
    "\n",
    "# Plot for x_label\n",
    "data_x = pd.DataFrame({'value': df[x_label].dropna(), 'dummy': 'x'})\n",
    "sns.violinplot(data=data_x, x='dummy', y='value', ax=ax1, color='#3b82f6', inner='box')\n",
    "ax1.set_title(f'{x_label}\\nDistribution', fontweight='bold')\n",
    "ax1.set_xlabel('')\n",
    "ax1.set_ylabel('Value', fontweight='bold')\n",
    "ax1.tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)\n",
    "\n",
    "# Plot for y_label\n",
    "data_y = pd.DataFrame({'value': df[y_label].dropna(), 'dummy': 'y'})\n",
    "sns.violinplot(data=data_y, x='dummy', y='value', ax=ax2, color='#ef4444', inner='box')\n",
    "ax2.set_title(f'{y_label}\\nDistribution', fontweight='bold')\n",
    "ax2.set_xlabel('')\n",
    "ax2.set_ylabel('')\n",
    "ax2.tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)\n",
    "\n",
    "# Add grid\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.suptitle(f'Side-by-Side Distribution Comparison: {x_label} vs {y_label}', \n",
    "                fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7269bce5",
   "metadata": {},
   "source": [
    "### Training & model performance visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0820bd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(df[x_label], df[y_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a199ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_value = 240000\n",
    "\n",
    "predicted_value = model.predict(scaler_x.transform(input_value))\n",
    "inverse_transform_output = scaler_y.inverse_transform(predicted_value)\n",
    "\n",
    "print(f'Predicted: {predicted_value} transform backed to {inverse_transform_output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e424b702",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'f(x) = {model.get_slope()}x + {model.get_y_intercept()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6214c311",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ab59d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_linear_regression_results(df, model, scaler_x, scaler_y, x_label, y_label, title=\"Linear Regression Results\", figsize=(10, 6)):\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    x_scaled = df[x_label].values\n",
    "    y_scaled = df[y_label].values\n",
    "    \n",
    "    x_original = scaler_x.inverse_transform(x_scaled)\n",
    "    y_original = scaler_y.inverse_transform(y_scaled)\n",
    "    \n",
    "    ax.scatter(x_original, y_original, alpha=0.6, color='blue', label='Data Points')\n",
    "    \n",
    "    x_range_scaled = np.linspace(x_scaled.min(), x_scaled.max(), 100)\n",
    "    y_pred_scaled = model.predict(x_range_scaled)\n",
    "    \n",
    "    x_range_original = scaler_x.inverse_transform(x_range_scaled)\n",
    "    y_pred_original = scaler_y.inverse_transform(y_pred_scaled)\n",
    "    \n",
    "    ax.plot(x_range_original, y_pred_original, color='red', linewidth=2, \n",
    "            label=f'Fitted Line (y = {scaler_x.inverse_transform(model.get_slope()):.2f}x + {scaler_y.inverse_transform(model.get_y_intercept()):.2f})')\n",
    "    \n",
    "    ax.set_xlabel(x_label.title())\n",
    "    ax.set_ylabel(y_label.title())\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Get final metrics for display\n",
    "    history = model.get_history()\n",
    "    final_mse = history['MSE'].iloc[-1]\n",
    "    final_rmse = history['RMSE'].iloc[-1] if 'RMSE' in history.columns else None\n",
    "    final_mae = history['MAE'].iloc[-1] if 'MAE' in history.columns else None\n",
    "    final_r2 = history['R^2'].iloc[-1] if 'R^2' in history.columns else None\n",
    "\n",
    "    # Create metrics text\n",
    "    metrics_text = f'Final MSE: {final_mse:.6f}'\n",
    "\n",
    "    if final_rmse is not None:\n",
    "        metrics_text += f'\\nFinal RMSE: {final_rmse:.6f}'\n",
    "    \n",
    "    if final_mae is not None:\n",
    "        metrics_text += f'\\nFinal MAE: {final_mae:.6f}'\n",
    "\n",
    "    if final_r2 is not None:\n",
    "        metrics_text += f'\\nR^2: {final_r2:.4f}'\n",
    "\n",
    "    ax.text(0.02, 0.98, metrics_text, \n",
    "        transform=ax.transAxes, verticalalignment='top',\n",
    "        bbox=dict(boxstyle='round', facecolor='white', alpha=0.8)\n",
    "    )\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "def plot_training_history(model, figsize=(15, 10)):\n",
    "\n",
    "    history = model.get_history()\n",
    "    epochs = range(1, len(history) + 1)\n",
    "\n",
    "    available_metrics = []\n",
    "    metric_colors = {\n",
    "        'MSE': 'green',\n",
    "        'RMSE': 'orange', \n",
    "        'MAE': 'purple',\n",
    "        'MAPE': 'brown',\n",
    "        'R^2': 'pink',\n",
    "        'Huber_Loss': 'cyan'\n",
    "    }\n",
    "    \n",
    "    for metric in metric_colors.keys():\n",
    "        if metric in history.columns:\n",
    "            available_metrics.append(metric)\n",
    "\n",
    "    n_metrics = len(available_metrics)\n",
    "    n_param_plots = 2\n",
    "\n",
    "    total_plots = n_metrics + n_param_plots\n",
    "    \n",
    "    # Determine optimal subplot arrangement\n",
    "    if total_plots <= 3:\n",
    "        rows, cols = 1, total_plots\n",
    "        fig_height = 5\n",
    "\n",
    "    elif total_plots <= 6:\n",
    "        rows, cols = 2, 3\n",
    "        fig_height = 8\n",
    "\n",
    "    else:\n",
    "        rows, cols = 3, 3\n",
    "        fig_height = 12\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(figsize[0], fig_height))\n",
    "    \n",
    "    # Handle case when there's only one subplot\n",
    "    if total_plots == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    elif rows == 1:\n",
    "        axes = axes if hasattr(axes, '__len__') else [axes]\n",
    "\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    plot_idx = 0\n",
    "\n",
    "    for metric in available_metrics:\n",
    "        ax = axes[plot_idx]\n",
    "        ax.plot(epochs, history[metric], color=metric_colors[metric], linewidth=2)\n",
    "        ax.set_title(f'Training {metric}')\n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.set_ylabel(metric)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add final value annotation\n",
    "        final_value = history[metric].iloc[-1]\n",
    "        if metric == 'MAPE':\n",
    "            ax.text(0.02, 0.95, f'Final: {final_value:.2f}%', \n",
    "                   transform=ax.transAxes, verticalalignment='top',\n",
    "                   bbox=dict(boxstyle='round', facecolor='white', alpha=0.7, pad=0.3))\n",
    "        elif metric == 'R^2':\n",
    "            ax.text(0.02, 0.95, f'Final: {final_value:.4f}', \n",
    "                   transform=ax.transAxes, verticalalignment='top',\n",
    "                   bbox=dict(boxstyle='round', facecolor='white', alpha=0.7, pad=0.3))\n",
    "        else:\n",
    "            ax.text(0.02, 0.95, f'Final: {final_value:.6f}', \n",
    "                   transform=ax.transAxes, verticalalignment='top',\n",
    "                   bbox=dict(boxstyle='round', facecolor='white', alpha=0.7, pad=0.3))\n",
    "        \n",
    "        plot_idx += 1\n",
    "    \n",
    "    # Plot slope history\n",
    "    if plot_idx < len(axes):\n",
    "        ax = axes[plot_idx]\n",
    "        ax.plot(epochs, history['slope'], color='blue', linewidth=2)\n",
    "        ax.set_title('Slope Evolution')\n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.set_ylabel('Slope (θ₁)')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        final_slope = history['slope'].iloc[-1]\n",
    "        ax.text(0.02, 0.95, f'Final: {final_slope:.6f}', \n",
    "               transform=ax.transAxes, verticalalignment='top',\n",
    "               bbox=dict(boxstyle='round', facecolor='white', alpha=0.7, pad=0.3))\n",
    "        \n",
    "        plot_idx += 1\n",
    "    \n",
    "    # Plot y-intercept history\n",
    "    if plot_idx < len(axes):\n",
    "        ax = axes[plot_idx]\n",
    "        ax.plot(epochs, history['y_intercept'], color='red', linewidth=2)\n",
    "        ax.set_title('Y-Intercept Evolution')\n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.set_ylabel('Y-Intercept (θ₀)')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        final_intercept = history['y_intercept'].iloc[-1]\n",
    "        ax.text(0.02, 0.95, f'Final: {final_intercept:.6f}', \n",
    "               transform=ax.transAxes, verticalalignment='top',\n",
    "               bbox=dict(boxstyle='round', facecolor='white', alpha=0.7, pad=0.3))\n",
    "        \n",
    "        plot_idx += 1\n",
    "    \n",
    "    # Hide any unused subplots\n",
    "    for i in range(plot_idx, len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig, axes\n",
    "\n",
    "\n",
    "def plot_error_comparison(model, figsize=(12, 8)):\n",
    "    \"\"\"\n",
    "    Plot multiple error metrics on the same chart for easy comparison\n",
    "    \"\"\"\n",
    "    history = model.get_history()\n",
    "    epochs = range(1, len(history) + 1)\n",
    "    \n",
    "    # Define which metrics to compare (exclude R^2 as it has different scale)\n",
    "    error_metrics = ['MSE', 'RMSE', 'MAE', 'Huber_Loss']\n",
    "    available_error_metrics = [m for m in error_metrics if m in history.columns]\n",
    "    \n",
    "    if len(available_error_metrics) < 2:\n",
    "        print(\"Not enough error metrics available for comparison plot\")\n",
    "        return None, None\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)\n",
    "    \n",
    "    # Plot 1: All error metrics\n",
    "    colors = ['green', 'orange', 'purple', 'cyan']\n",
    "    for i, metric in enumerate(available_error_metrics):\n",
    "        ax1.plot(epochs, history[metric], color=colors[i % len(colors)], \n",
    "                linewidth=2, label=metric)\n",
    "    \n",
    "    ax1.set_title('Error Metrics Comparison')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Error Value')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_yscale('log')  # Log scale for better visualization\n",
    "    \n",
    "    # Plot 2: R^2 if available\n",
    "    if 'R^2' in history.columns:\n",
    "        ax2.plot(epochs, history['R^2'], color='pink', linewidth=2)\n",
    "        ax2.set_title('R^2 Score Evolution')\n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax2.set_ylabel('R^2 Score')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        ax2.axhline(y=0, color='k', linestyle='--', alpha=0.5)\n",
    "        ax2.axhline(y=1, color='k', linestyle='--', alpha=0.5)\n",
    "    else:\n",
    "        # Plot MAPE if R^2 not available\n",
    "        if 'MAPE' in history.columns:\n",
    "            ax2.plot(epochs, history['MAPE'], color='brown', linewidth=2)\n",
    "            ax2.set_title('MAPE Evolution')\n",
    "            ax2.set_xlabel('Epoch')\n",
    "            ax2.set_ylabel('MAPE (%)')\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "        else:\n",
    "            ax2.text(0.5, 0.5, 'No additional metrics\\navailable for plotting', \n",
    "                    ha='center', va='center', transform=ax2.transAxes)\n",
    "            ax2.set_title('Additional Metrics')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig, (ax1, ax2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48838516",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_linear_regression_results(\n",
    "    df, model, scaler_x, scaler_y, x_label, y_label,\n",
    "    title=\"Car Price vs Kilometers - Linear Regression\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53e17e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f648ade3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_error_comparison(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6574cf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.get_history()\n",
    "\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot 1: 3D Loss Surface with Training Path\n",
    "ax1 = fig.add_subplot(131, projection='3d')\n",
    "\n",
    "ax1.plot(history['slope'], history['y_intercept'], history['MSE'], \n",
    "         'r-', linewidth=2, alpha=0.8, label='Training Path')\n",
    "\n",
    "ax1.scatter(history['slope'].iloc[0], history['y_intercept'].iloc[0], history['MSE'].iloc[0], \n",
    "           color='green', s=100, label='Start')\n",
    "ax1.scatter(history['slope'].iloc[-1], history['y_intercept'].iloc[-1], history['MSE'].iloc[-1], \n",
    "           color='red', s=100, label='End')\n",
    "\n",
    "ax1.set_xlabel('Slope')\n",
    "ax1.set_ylabel('Y-Intercept')\n",
    "ax1.set_zlabel('MSE Loss')\n",
    "ax1.set_title('3D Training Trajectory')\n",
    "ax1.legend()\n",
    "\n",
    "fig2 = plt.figure(figsize=(12, 10))\n",
    "\n",
    "slope_range = np.linspace(min(history['slope']) - 0.5, max(history['slope']) + 0.5, 50)\n",
    "intercept_range = np.linspace(min(history['y_intercept']) - 0.5, max(history['y_intercept']) + 0.5, 50)\n",
    "Slope_grid, Intercept_grid = np.meshgrid(slope_range, intercept_range)\n",
    "\n",
    "X_train = df[x_label].values\n",
    "y_train = df[y_label].values\n",
    "\n",
    "MSE_grid = np.zeros_like(Slope_grid)\n",
    "for i in range(len(slope_range)):\n",
    "    for j in range(len(intercept_range)):\n",
    "        predictions = Slope_grid[j, i] * X_train + Intercept_grid[j, i]\n",
    "        MSE_grid[j, i] = np.mean((y_train - predictions) ** 2)\n",
    "\n",
    "# Contour plot (2D projection)\n",
    "ax2 = fig2.add_subplot(122)\n",
    "contour = ax2.contour(Slope_grid, Intercept_grid, MSE_grid, levels=20, cmap='viridis')\n",
    "ax2.clabel(contour, inline=True, fontsize=8)\n",
    "ax2.plot(history['slope'], history['y_intercept'], 'r-', linewidth=2, label='Training Path')\n",
    "ax2.scatter(history['slope'].iloc[0], history['y_intercept'].iloc[0], color='green', s=100, label='Start')\n",
    "ax2.scatter(history['slope'].iloc[-1], history['y_intercept'].iloc[-1], color='red', s=100, label='End')\n",
    "ax2.set_xlabel('Slope')\n",
    "ax2.set_ylabel('Y-Intercept')\n",
    "ax2.set_title('Loss Contours with Training Path')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda310-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
